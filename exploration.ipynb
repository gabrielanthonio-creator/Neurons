{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron-Level Analysis of Transformer Attention Patterns\n",
    "\n",
    "**Interactive Exploration Notebook**\n",
    "\n",
    "This notebook walks through each section of the analysis interactively:\n",
    "1. Load the model and extract raw attention weights\n",
    "2. Visualise per-head attention heatmaps\n",
    "3. Analyse sentiment-correlated heads\n",
    "4. Probe gender-bias differential attention\n",
    "5. Identify instruction-following heads\n",
    "6. Explore FFN neuron activations\n",
    "7. Run TransformerLens ablations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from attention_analyzer import (\n",
    "    load_model, get_attentions, head_entropy,\n",
    "    top_attended_tokens, sentiment_head_scores,\n",
    "    bias_differential_attention, instruction_head_scores,\n",
    "    get_ffn_activations, top_active_neurons,\n",
    ")\n",
    "from visualizer import (\n",
    "    plot_attention_heatmap, plot_layer_heads, plot_entropy_heatmap,\n",
    "    plot_sentiment_heatmap, plot_bias_heatmap, plot_instruction_heatmap,\n",
    "    plot_neuron_activations, plot_head_summary,\n",
    ")\n",
    "from transformerlens_analysis import (\n",
    "    load_hooked_model, logit_lens, full_ablation_matrix,\n",
    "    tl_get_attention_patterns, tl_diagnostic,\n",
    ")\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to 'gpt2-medium', 'gpt2-large', or 'meta-llama/Llama-2-7b-hf'\n",
    "# For Llama-2 you will need to accept the HuggingFace license and set HF_TOKEN\n",
    "MODEL_NAME = 'gpt2'\n",
    "\n",
    "tokenizer, model = load_model(MODEL_NAME)\n",
    "n_layers = model.config.num_hidden_layers\n",
    "n_heads  = model.config.num_attention_heads\n",
    "print(f'Layers: {n_layers}  |  Heads: {n_heads}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Attention Extraction & Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = 'The quick brown fox jumps over the lazy dog near the river bank.'\n",
    "\n",
    "tokens, attn = get_attentions(TEXT, tokenizer, model)\n",
    "print('Tokens:', tokens)\n",
    "print('Attention shape:', attn.shape)  # (layers, heads, seq, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Single head heatmap ---\n",
    "# Try different layer/head combinations to see specialised patterns\n",
    "LAYER = 5\n",
    "HEAD  = 2\n",
    "\n",
    "fig = plot_attention_heatmap(attn, tokens, layer=LAYER, head=HEAD, save=False)\n",
    "plt.show()\n",
    "\n",
    "# Top tokens attended to by this head\n",
    "top = top_attended_tokens(attn, tokens, LAYER, HEAD, top_k=5)\n",
    "print(f'Top attended tokens (L{LAYER} H{HEAD}):', top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- All heads in one layer ---\n",
    "fig = plot_layer_heads(attn, tokens, layer=LAYER, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Head Entropy — Which Heads Are Most Focused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = head_entropy(attn)\n",
    "\n",
    "# Print 5 most focused heads\n",
    "flat_idx = np.argsort(entropy.flatten())\n",
    "print('Most focused heads (lowest entropy):')\n",
    "for i in flat_idx[:5]:\n",
    "    l, h = i // n_heads, i % n_heads\n",
    "    print(f'  Layer {l:2d}  Head {h:2d}  entropy={entropy[l,h]:.4f}')\n",
    "\n",
    "fig = plot_entropy_heatmap(entropy, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment-Correlated Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_DATA = [\n",
    "    ('This movie was absolutely wonderful and I loved every moment.',            1),\n",
    "    ('The product quality is excellent and delivery was superb.',               1),\n",
    "    ('I had an amazing experience at this restaurant, highly recommended.',     1),\n",
    "    ('The customer service was fantastic and resolved my issue instantly.',     1),\n",
    "    ('What a beautiful day — everything felt positive and joyful.',             1),\n",
    "    ('This service was terrible and I am deeply disappointed.',                 0),\n",
    "    ('The food was disgusting and the staff were rude and unhelpful.',          0),\n",
    "    ('Worst experience ever. I will never return to this horrible place.',      0),\n",
    "    ('The product broke immediately — awful quality and poor design.',           0),\n",
    "    ('I had a dreadful time and regret spending money on this.',                0),\n",
    "]\n",
    "\n",
    "corpus = [t for t, _ in SENTIMENT_DATA]\n",
    "labels = [l for _, l in SENTIMENT_DATA]\n",
    "\n",
    "sent_corr = sentiment_head_scores(corpus, labels, tokenizer, model)\n",
    "\n",
    "# Top 5 sentiment-correlated heads\n",
    "top_idx = np.argsort(np.abs(sent_corr.flatten()))[::-1][:5]\n",
    "print('Top sentiment-correlated heads:')\n",
    "for i in top_idx:\n",
    "    l, h = i // n_heads, i % n_heads\n",
    "    print(f'  Layer {l:2d}  Head {h:2d}  r={sent_corr[l,h]:+.4f}')\n",
    "\n",
    "fig = plot_sentiment_heatmap(sent_corr, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gender-Bias Differential Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add custom probe pairs here\n",
    "CUSTOM_PAIRS = [\n",
    "    ('The scientist presented his findings at the conference.',\n",
    "     'The scientist presented her findings at the conference.'),\n",
    "    ('The pilot landed his aircraft safely.',\n",
    "     'The pilot landed her aircraft safely.'),\n",
    "    ('The lawyer argued his case before the judge.',\n",
    "     'The lawyer argued her case before the judge.'),\n",
    "]\n",
    "\n",
    "bias_diff = bias_differential_attention(tokenizer, model, pairs=CUSTOM_PAIRS)\n",
    "\n",
    "top_idx = np.argsort(bias_diff.flatten())[::-1][:5]\n",
    "print('Top bias-sensitive heads:')\n",
    "for i in top_idx:\n",
    "    l, h = i // n_heads, i % n_heads\n",
    "    print(f'  Layer {l:2d}  Head {h:2d}  Δ={bias_diff[l,h]:.5f}')\n",
    "\n",
    "fig = plot_bias_heatmap(bias_diff, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Instruction-Following Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_scores = instruction_head_scores(tokenizer=tokenizer, model=model)\n",
    "\n",
    "top_idx = np.argsort(inst_scores.flatten())[::-1][:5]\n",
    "print('Top instruction-following heads:')\n",
    "for i in top_idx:\n",
    "    l, h = i // n_heads, i % n_heads\n",
    "    print(f'  Layer {l:2d}  Head {h:2d}  surplus={inst_scores[l,h]:+.5f}')\n",
    "\n",
    "fig = plot_instruction_heatmap(inst_scores, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FFN Neuron Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURON_TEXT = 'Artificial intelligence is transforming the world of science.'\n",
    "\n",
    "activations = get_ffn_activations(NEURON_TEXT, tokenizer, model)\n",
    "print(f'Activation layers captured: {sorted(activations.keys())}')\n",
    "\n",
    "# Inspect layer 6\n",
    "INSPECT_LAYER = 6\n",
    "top_neurons = top_active_neurons(activations, layer=INSPECT_LAYER, top_k=10)\n",
    "print(f'\\nTop 10 neurons in layer {INSPECT_LAYER}:')\n",
    "for neuron, score in top_neurons:\n",
    "    print(f'  Neuron {neuron:4d}  mean|act|={score:.5f}')\n",
    "\n",
    "fig = plot_neuron_activations(top_neurons, layer=INSPECT_LAYER,\n",
    "                              title_suffix='(AI text)', save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TransformerLens — Logit Lens & Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TransformerLens is available\n",
    "diag = tl_diagnostic('gpt2')\n",
    "print('TL Diagnostic:', diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If TL is available, run the logit lens\n",
    "tl_model = load_hooked_model('gpt2')\n",
    "\n",
    "if tl_model is not None:\n",
    "    preds = logit_lens(tl_model, 'The Eiffel Tower is located in', top_k=5)\n",
    "    print('Logit Lens predictions by layer:')\n",
    "    for layer, top in preds.items():\n",
    "        print(f'  Layer {layer:2d}: {[(t, round(p,3)) for t,p in top]}')\n",
    "else:\n",
    "    print('TL not available — mock preds shown')\n",
    "    preds = logit_lens(None, '', top_k=5)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head ablation — WARNING: this is slow (L x H forward passes)\n",
    "# Restrict to 3 layers for a quick demo\n",
    "\n",
    "ABLATION_TEXT = 'The capital of France is'\n",
    "ABLATION_LAYERS = [0, 5, 11]  # First, middle, last layer\n",
    "\n",
    "delta_matrix = full_ablation_matrix(\n",
    "    model=tl_model,\n",
    "    text=ABLATION_TEXT,\n",
    "    layers=ABLATION_LAYERS,\n",
    ")\n",
    "\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "sns.heatmap(\n",
    "    delta_matrix[ABLATION_LAYERS],\n",
    "    ax=ax, cmap='Reds', annot=True, fmt='.3f',\n",
    "    xticklabels=[f'H{h}' for h in range(delta_matrix.shape[1])],\n",
    "    yticklabels=[f'L{l}' for l in ABLATION_LAYERS],\n",
    "    cbar_kws={'label': 'Δ Loss (positive = head matters)'}\n",
    ")\n",
    "ax.set_title('Head Ablation — ΔLoss', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_head_summary(\n",
    "    entropy=entropy,\n",
    "    sentiment=sent_corr,\n",
    "    bias=bias_diff,\n",
    "    instruction=inst_scores,\n",
    "    top_n=5,\n",
    "    save=False,\n",
    ")\n",
    "plt.show()\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
